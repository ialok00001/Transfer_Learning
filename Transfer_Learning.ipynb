{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ELQgnfucEv3v"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import models"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Choose a Pre-trained Model\n",
        "pretrained_model = models.resnet18(pretrained=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GdtHyGZyE9pi",
        "outputId": "0abc47df-c8d9-4cdb-875c-2cb8cd6924ca"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 145MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Prepare Your Dataset\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),  # Resize images to fit the input size of ResNet\n",
        "    transforms.ToTensor(),           # Convert images to PyTorch tensors\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize images\n",
        "])"
      ],
      "metadata": {
        "id": "92X6KRTtFByt"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainset = torchvision.datasets.CIFAR10(root='/content', train=True, download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=32, shuffle=True)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='/content', train=False, download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=32, shuffle=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LSjcSahrFbwK",
        "outputId": "a2e50a5d-fea5-4113-c89e-341fba79d70e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to /content/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:01<00:00, 101571210.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /content/cifar-10-python.tar.gz to /content\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_classes = 10"
      ],
      "metadata": {
        "id": "C568ZgOyFxMf"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pretrained_model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8F7w5nJxJZ9_",
        "outputId": "1a1b8c0d-5817-445e-fce9-85529dbf26b0"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (layer1): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pretrained_model.fc.in_features"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QsrrgXXLKmPM",
        "outputId": "bf1df2c3-5e85-4180-b991-332582dd8413"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "512"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_ftrs = pretrained_model.fc.in_features"
      ],
      "metadata": {
        "id": "PmoFtLPIG_H8"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pretrained_model.fc = nn.Linear(num_ftrs, num_classes, bias = False)"
      ],
      "metadata": {
        "id": "pxOjLX9CKvxV"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pretrained_model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yitR-a1YLEFP",
        "outputId": "e765f8c8-6b6c-42f1-c3c2-f29eb9ac41ed"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (layer1): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (fc): Linear(in_features=512, out_features=10, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(pretrained_model.parameters(), lr=0.001, momentum=0.9)"
      ],
      "metadata": {
        "id": "xBPC_Ux3K1Mc"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 6: Training Loop\n",
        "num_epochs = 5\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "pretrained_model = pretrained_model.to(device)"
      ],
      "metadata": {
        "id": "P-xpHr9dK9r6"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E_7zUPJWMbNg",
        "outputId": "888f4943-7bd7-4c9c-e3e4-3c27b2114328"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(num_epochs):\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        inputs, labels = data[0].to(device), data[1].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = pretrained_model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        if i % 100 == 99:\n",
        "            print('[%d, %5d] loss: %.3f' %\n",
        "                  (epoch + 1, i + 1, running_loss / 100))\n",
        "            running_loss = 0.0\n",
        "\n",
        "print(\"Finished Training\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NVjHIqq9LUCC",
        "outputId": "b677f9bd-661d-4b2b-e792-6eb2bd996f4e"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1,   100] loss: 1.418\n",
            "[1,   200] loss: 0.634\n",
            "[1,   300] loss: 0.458\n",
            "[1,   400] loss: 0.394\n",
            "[1,   500] loss: 0.329\n",
            "[1,   600] loss: 0.323\n",
            "[1,   700] loss: 0.303\n",
            "[1,   800] loss: 0.286\n",
            "[1,   900] loss: 0.273\n",
            "[1,  1000] loss: 0.258\n",
            "[1,  1100] loss: 0.265\n",
            "[1,  1200] loss: 0.276\n",
            "[1,  1300] loss: 0.258\n",
            "[1,  1400] loss: 0.239\n",
            "[1,  1500] loss: 0.231\n",
            "[2,   100] loss: 0.160\n",
            "[2,   200] loss: 0.162\n",
            "[2,   300] loss: 0.151\n",
            "[2,   400] loss: 0.147\n",
            "[2,   500] loss: 0.152\n",
            "[2,   600] loss: 0.156\n",
            "[2,   700] loss: 0.148\n",
            "[2,   800] loss: 0.153\n",
            "[2,   900] loss: 0.139\n",
            "[2,  1000] loss: 0.160\n",
            "[2,  1100] loss: 0.136\n",
            "[2,  1200] loss: 0.146\n",
            "[2,  1300] loss: 0.153\n",
            "[2,  1400] loss: 0.152\n",
            "[2,  1500] loss: 0.131\n",
            "[3,   100] loss: 0.086\n",
            "[3,   200] loss: 0.089\n",
            "[3,   300] loss: 0.078\n",
            "[3,   400] loss: 0.087\n",
            "[3,   500] loss: 0.080\n",
            "[3,   600] loss: 0.076\n",
            "[3,   700] loss: 0.087\n",
            "[3,   800] loss: 0.088\n",
            "[3,   900] loss: 0.082\n",
            "[3,  1000] loss: 0.075\n",
            "[3,  1100] loss: 0.077\n",
            "[3,  1200] loss: 0.085\n",
            "[3,  1300] loss: 0.084\n",
            "[3,  1400] loss: 0.081\n",
            "[3,  1500] loss: 0.080\n",
            "[4,   100] loss: 0.049\n",
            "[4,   200] loss: 0.044\n",
            "[4,   300] loss: 0.040\n",
            "[4,   400] loss: 0.038\n",
            "[4,   500] loss: 0.047\n",
            "[4,   600] loss: 0.045\n",
            "[4,   700] loss: 0.044\n",
            "[4,   800] loss: 0.046\n",
            "[4,   900] loss: 0.040\n",
            "[4,  1000] loss: 0.048\n",
            "[4,  1100] loss: 0.038\n",
            "[4,  1200] loss: 0.045\n",
            "[4,  1300] loss: 0.041\n",
            "[4,  1400] loss: 0.046\n",
            "[4,  1500] loss: 0.042\n",
            "[5,   100] loss: 0.030\n",
            "[5,   200] loss: 0.023\n",
            "[5,   300] loss: 0.029\n",
            "[5,   400] loss: 0.027\n",
            "[5,   500] loss: 0.028\n",
            "[5,   600] loss: 0.028\n",
            "[5,   700] loss: 0.026\n",
            "[5,   800] loss: 0.029\n",
            "[5,   900] loss: 0.024\n",
            "[5,  1000] loss: 0.028\n",
            "[5,  1100] loss: 0.026\n",
            "[5,  1200] loss: 0.027\n",
            "[5,  1300] loss: 0.029\n",
            "[5,  1400] loss: 0.030\n",
            "[5,  1500] loss: 0.028\n",
            "Finished Training\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 7: Evaluation\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data[0].to(device), data[1].to(device)\n",
        "        outputs = pretrained_model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()"
      ],
      "metadata": {
        "id": "2z98-T4tL0Zd"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Accuracy of the network on the 10000 test images: %d %%' % (100 * correct / total))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-34J0gLBQnZW",
        "outputId": "4fe00c97-4e08-4cad-b150-7a6e26c0ca8e"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the network on the 10000 test images: 94 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Now with model parameters freezed"
      ],
      "metadata": {
        "id": "1z5MRoIBRI_Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Choose a Pre-trained Model\n",
        "pretrained_model = models.resnet18(pretrained=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kkHaZOWRQ-Lj",
        "outputId": "ed25ee1c-2227-4ead-97c9-9da86221c638"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pretrained_model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Go-iuXYIRUas",
        "outputId": "ad40007d-6d38-4751-e1ef-4ccb8d0f5018"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (layer1): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_ftrs = pretrained_model.fc.in_features"
      ],
      "metadata": {
        "id": "IKGGrW4mRdry"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Freeze parameters of pre-trained layers\n",
        "for param in pretrained_model.parameters():\n",
        "    param.requires_grad = False"
      ],
      "metadata": {
        "id": "O6z7NU6URi5A"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace the fully connected layer with a new one\n",
        "pretrained_model.fc = nn.Linear(num_ftrs, num_classes)"
      ],
      "metadata": {
        "id": "JIFzIN9gRnoC"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters of the newly added fully connected layer will be updated during training\n",
        "parameters_to_update = pretrained_model.fc.parameters()\n",
        "optimizer = optim.SGD(parameters_to_update, lr=0.001, momentum=0.9)"
      ],
      "metadata": {
        "id": "1R6Tc0TGRrnk"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 6: Training Loop\n",
        "num_epochs = 5\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "pretrained_model = pretrained_model.to(device)"
      ],
      "metadata": {
        "id": "xcJTRTzXRxhS"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ehqGoIOR1sc",
        "outputId": "55d509f0-0fb4-434f-9d02-4cd69519c36e"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(num_epochs):\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        inputs, labels = data[0].to(device), data[1].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = pretrained_model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        if i % 100 == 99:\n",
        "            print('[%d, %5d] loss: %.3f' %\n",
        "                  (epoch + 1, i + 1, running_loss / 100))\n",
        "            running_loss = 0.0\n",
        "\n",
        "print(\"Finished Training\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jp6CN6mER3XH",
        "outputId": "5d9fe2f8-57df-489e-e26d-a13c87c6cdd9"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1,   100] loss: 1.765\n",
            "[1,   200] loss: 1.180\n",
            "[1,   300] loss: 1.008\n",
            "[1,   400] loss: 0.922\n",
            "[1,   500] loss: 0.842\n",
            "[1,   600] loss: 0.796\n",
            "[1,   700] loss: 0.772\n",
            "[1,   800] loss: 0.779\n",
            "[1,   900] loss: 0.732\n",
            "[1,  1000] loss: 0.739\n",
            "[1,  1100] loss: 0.732\n",
            "[1,  1200] loss: 0.700\n",
            "[1,  1300] loss: 0.713\n",
            "[1,  1400] loss: 0.672\n",
            "[1,  1500] loss: 0.705\n",
            "[2,   100] loss: 0.659\n",
            "[2,   200] loss: 0.640\n",
            "[2,   300] loss: 0.657\n",
            "[2,   400] loss: 0.672\n",
            "[2,   500] loss: 0.671\n",
            "[2,   600] loss: 0.677\n",
            "[2,   700] loss: 0.673\n",
            "[2,   800] loss: 0.656\n",
            "[2,   900] loss: 0.640\n",
            "[2,  1000] loss: 0.633\n",
            "[2,  1100] loss: 0.639\n",
            "[2,  1200] loss: 0.654\n",
            "[2,  1300] loss: 0.650\n",
            "[2,  1400] loss: 0.653\n",
            "[2,  1500] loss: 0.663\n",
            "[3,   100] loss: 0.631\n",
            "[3,   200] loss: 0.635\n",
            "[3,   300] loss: 0.625\n",
            "[3,   400] loss: 0.648\n",
            "[3,   500] loss: 0.606\n",
            "[3,   600] loss: 0.612\n",
            "[3,   700] loss: 0.650\n",
            "[3,   800] loss: 0.630\n",
            "[3,   900] loss: 0.623\n",
            "[3,  1000] loss: 0.647\n",
            "[3,  1100] loss: 0.612\n",
            "[3,  1200] loss: 0.628\n",
            "[3,  1300] loss: 0.616\n",
            "[3,  1400] loss: 0.621\n",
            "[3,  1500] loss: 0.627\n",
            "[4,   100] loss: 0.619\n",
            "[4,   200] loss: 0.611\n",
            "[4,   300] loss: 0.611\n",
            "[4,   400] loss: 0.595\n",
            "[4,   500] loss: 0.616\n",
            "[4,   600] loss: 0.611\n",
            "[4,   700] loss: 0.595\n",
            "[4,   800] loss: 0.612\n",
            "[4,   900] loss: 0.613\n",
            "[4,  1000] loss: 0.604\n",
            "[4,  1100] loss: 0.608\n",
            "[4,  1200] loss: 0.584\n",
            "[4,  1300] loss: 0.603\n",
            "[4,  1400] loss: 0.637\n",
            "[4,  1500] loss: 0.600\n",
            "[5,   100] loss: 0.571\n",
            "[5,   200] loss: 0.606\n",
            "[5,   300] loss: 0.611\n",
            "[5,   400] loss: 0.579\n",
            "[5,   500] loss: 0.587\n",
            "[5,   600] loss: 0.622\n",
            "[5,   700] loss: 0.633\n",
            "[5,   800] loss: 0.608\n",
            "[5,   900] loss: 0.596\n",
            "[5,  1000] loss: 0.628\n",
            "[5,  1100] loss: 0.598\n",
            "[5,  1200] loss: 0.590\n",
            "[5,  1300] loss: 0.577\n",
            "[5,  1400] loss: 0.597\n",
            "[5,  1500] loss: 0.629\n",
            "Finished Training\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 7: Evaluation\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data[0].to(device), data[1].to(device)\n",
        "        outputs = pretrained_model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()"
      ],
      "metadata": {
        "id": "gw3eEkJSR6f_"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Accuracy of the network on the 10000 test images: %d %%' % (100 * correct / total))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z5hPveHESHsR",
        "outputId": "c6428a74-9961-4cb3-ed11-bdf2db1b4ed6"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the network on the 10000 test images: 78 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Freeze model parameters and add more fully connected layers"
      ],
      "metadata": {
        "id": "MgdqDZSSSIum"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Choose a Pre-trained Model\n",
        "pretrained_model = models.resnet18(pretrained=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wuGxqNkJmB9z",
        "outputId": "e6b48691-96c6-437c-8d2e-025a1f3f33fe"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 140MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Freeze Pre-trained Model and Add More Fully Connected Layers\n",
        "num_classes = 10  # Number of classes in CIFAR-10\n",
        "num_ftrs = pretrained_model.fc.in_features\n",
        "\n",
        "pretrained_model.fc = nn.Linear(num_ftrs, num_classes)"
      ],
      "metadata": {
        "id": "bKIiqoB5nATa"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pretrained_model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kialirL4qQ7q",
        "outputId": "2a39eb41-a5be-4ad7-b538-dace47502d6d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (layer1): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainable_layer_names = [\"layer4\", \"avgpool\", \"fc\"]"
      ],
      "metadata": {
        "id": "6ht7fN1GtqB8"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Iterate through all the modules of the model\n",
        "for name, module in pretrained_model.named_children():\n",
        "    # Check if the module name is in the list of trainable_layer_names\n",
        "    if name in trainable_layer_names:\n",
        "        # If the module name is in the list, set requires_grad to True for all its parameters\n",
        "        for param in module.parameters():\n",
        "            param.requires_grad = True\n",
        "    else:\n",
        "        # If the module name is not in the list, freeze its parameters\n",
        "        for param in module.parameters():\n",
        "            param.requires_grad = False"
      ],
      "metadata": {
        "id": "TZ9vfx89uSTX"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify the status of requires_grad for each parameter\n",
        "for name, param in pretrained_model.named_parameters():\n",
        "    print(f'{name}: requires_grad={param.requires_grad}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jyee7gRKnmop",
        "outputId": "23158a01-2b71-4c9a-b278-1632433dc7a9"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "conv1.weight: requires_grad=False\n",
            "bn1.weight: requires_grad=False\n",
            "bn1.bias: requires_grad=False\n",
            "layer1.0.conv1.weight: requires_grad=False\n",
            "layer1.0.bn1.weight: requires_grad=False\n",
            "layer1.0.bn1.bias: requires_grad=False\n",
            "layer1.0.conv2.weight: requires_grad=False\n",
            "layer1.0.bn2.weight: requires_grad=False\n",
            "layer1.0.bn2.bias: requires_grad=False\n",
            "layer1.1.conv1.weight: requires_grad=False\n",
            "layer1.1.bn1.weight: requires_grad=False\n",
            "layer1.1.bn1.bias: requires_grad=False\n",
            "layer1.1.conv2.weight: requires_grad=False\n",
            "layer1.1.bn2.weight: requires_grad=False\n",
            "layer1.1.bn2.bias: requires_grad=False\n",
            "layer2.0.conv1.weight: requires_grad=False\n",
            "layer2.0.bn1.weight: requires_grad=False\n",
            "layer2.0.bn1.bias: requires_grad=False\n",
            "layer2.0.conv2.weight: requires_grad=False\n",
            "layer2.0.bn2.weight: requires_grad=False\n",
            "layer2.0.bn2.bias: requires_grad=False\n",
            "layer2.0.downsample.0.weight: requires_grad=False\n",
            "layer2.0.downsample.1.weight: requires_grad=False\n",
            "layer2.0.downsample.1.bias: requires_grad=False\n",
            "layer2.1.conv1.weight: requires_grad=False\n",
            "layer2.1.bn1.weight: requires_grad=False\n",
            "layer2.1.bn1.bias: requires_grad=False\n",
            "layer2.1.conv2.weight: requires_grad=False\n",
            "layer2.1.bn2.weight: requires_grad=False\n",
            "layer2.1.bn2.bias: requires_grad=False\n",
            "layer3.0.conv1.weight: requires_grad=False\n",
            "layer3.0.bn1.weight: requires_grad=False\n",
            "layer3.0.bn1.bias: requires_grad=False\n",
            "layer3.0.conv2.weight: requires_grad=False\n",
            "layer3.0.bn2.weight: requires_grad=False\n",
            "layer3.0.bn2.bias: requires_grad=False\n",
            "layer3.0.downsample.0.weight: requires_grad=False\n",
            "layer3.0.downsample.1.weight: requires_grad=False\n",
            "layer3.0.downsample.1.bias: requires_grad=False\n",
            "layer3.1.conv1.weight: requires_grad=False\n",
            "layer3.1.bn1.weight: requires_grad=False\n",
            "layer3.1.bn1.bias: requires_grad=False\n",
            "layer3.1.conv2.weight: requires_grad=False\n",
            "layer3.1.bn2.weight: requires_grad=False\n",
            "layer3.1.bn2.bias: requires_grad=False\n",
            "layer4.0.conv1.weight: requires_grad=True\n",
            "layer4.0.bn1.weight: requires_grad=True\n",
            "layer4.0.bn1.bias: requires_grad=True\n",
            "layer4.0.conv2.weight: requires_grad=True\n",
            "layer4.0.bn2.weight: requires_grad=True\n",
            "layer4.0.bn2.bias: requires_grad=True\n",
            "layer4.0.downsample.0.weight: requires_grad=True\n",
            "layer4.0.downsample.1.weight: requires_grad=True\n",
            "layer4.0.downsample.1.bias: requires_grad=True\n",
            "layer4.1.conv1.weight: requires_grad=True\n",
            "layer4.1.bn1.weight: requires_grad=True\n",
            "layer4.1.bn1.bias: requires_grad=True\n",
            "layer4.1.conv2.weight: requires_grad=True\n",
            "layer4.1.bn2.weight: requires_grad=True\n",
            "layer4.1.bn2.bias: requires_grad=True\n",
            "fc.weight: requires_grad=True\n",
            "fc.bias: requires_grad=True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = pretrained_model.to(device)\n",
        "print(device)\n",
        "\n",
        "# Step 5: Define Loss Function and Optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(pretrained_model.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# Step 6: Training Loop\n",
        "num_epochs = 5\n",
        "\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        inputs, labels = data[0].to(device), data[1].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = pretrained_model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        if i % 100 == 99:\n",
        "            print('[%d, %5d] loss: %.3f' %\n",
        "                  (epoch + 1, i + 1, running_loss / 100))\n",
        "            running_loss = 0.0\n",
        "\n",
        "print(\"Finished Training\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0y8bROSkSNXX",
        "outputId": "28acc4d5-c0f7-4f49-9bb4-7f06157be6ac"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n",
            "[1,   100] loss: 1.497\n",
            "[1,   200] loss: 0.800\n",
            "[1,   300] loss: 0.641\n",
            "[1,   400] loss: 0.553\n",
            "[1,   500] loss: 0.535\n",
            "[1,   600] loss: 0.516\n",
            "[1,   700] loss: 0.480\n",
            "[1,   800] loss: 0.430\n",
            "[1,   900] loss: 0.447\n",
            "[1,  1000] loss: 0.449\n",
            "[1,  1100] loss: 0.402\n",
            "[1,  1200] loss: 0.425\n",
            "[1,  1300] loss: 0.394\n",
            "[1,  1400] loss: 0.401\n",
            "[1,  1500] loss: 0.378\n",
            "[2,   100] loss: 0.314\n",
            "[2,   200] loss: 0.326\n",
            "[2,   300] loss: 0.284\n",
            "[2,   400] loss: 0.279\n",
            "[2,   500] loss: 0.294\n",
            "[2,   600] loss: 0.270\n",
            "[2,   700] loss: 0.270\n",
            "[2,   800] loss: 0.275\n",
            "[2,   900] loss: 0.286\n",
            "[2,  1000] loss: 0.285\n",
            "[2,  1100] loss: 0.268\n",
            "[2,  1200] loss: 0.259\n",
            "[2,  1300] loss: 0.277\n",
            "[2,  1400] loss: 0.259\n",
            "[2,  1500] loss: 0.271\n",
            "[3,   100] loss: 0.193\n",
            "[3,   200] loss: 0.197\n",
            "[3,   300] loss: 0.195\n",
            "[3,   400] loss: 0.203\n",
            "[3,   500] loss: 0.187\n",
            "[3,   600] loss: 0.182\n",
            "[3,   700] loss: 0.188\n",
            "[3,   800] loss: 0.191\n",
            "[3,   900] loss: 0.176\n",
            "[3,  1000] loss: 0.182\n",
            "[3,  1100] loss: 0.183\n",
            "[3,  1200] loss: 0.186\n",
            "[3,  1300] loss: 0.194\n",
            "[3,  1400] loss: 0.171\n",
            "[3,  1500] loss: 0.181\n",
            "[4,   100] loss: 0.126\n",
            "[4,   200] loss: 0.131\n",
            "[4,   300] loss: 0.123\n",
            "[4,   400] loss: 0.116\n",
            "[4,   500] loss: 0.121\n",
            "[4,   600] loss: 0.117\n",
            "[4,   700] loss: 0.116\n",
            "[4,   800] loss: 0.112\n",
            "[4,   900] loss: 0.114\n",
            "[4,  1000] loss: 0.111\n",
            "[4,  1100] loss: 0.122\n",
            "[4,  1200] loss: 0.124\n",
            "[4,  1300] loss: 0.123\n",
            "[4,  1400] loss: 0.117\n",
            "[4,  1500] loss: 0.108\n",
            "[5,   100] loss: 0.075\n",
            "[5,   200] loss: 0.067\n",
            "[5,   300] loss: 0.069\n",
            "[5,   400] loss: 0.080\n",
            "[5,   500] loss: 0.070\n",
            "[5,   600] loss: 0.075\n",
            "[5,   700] loss: 0.070\n",
            "[5,   800] loss: 0.069\n",
            "[5,   900] loss: 0.084\n",
            "[5,  1000] loss: 0.067\n",
            "[5,  1100] loss: 0.078\n",
            "[5,  1200] loss: 0.077\n",
            "[5,  1300] loss: 0.072\n",
            "[5,  1400] loss: 0.070\n",
            "[5,  1500] loss: 0.067\n",
            "Finished Training\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 7: Evaluation\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data[0].to(device), data[1].to(device)\n",
        "        outputs = pretrained_model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()"
      ],
      "metadata": {
        "id": "BwUTJMGLTBaf"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Accuracy of the network on the 10000 test images: %d %%' % (100 * correct / total))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pp41T3qIx70Y",
        "outputId": "0602cf17-a924-4a16-87fa-7dd5d7e5296f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the network on the 10000 test images: 89 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Now training the last two blocks of network"
      ],
      "metadata": {
        "id": "YGsvjCdkv-6Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Choose a Pre-trained Model\n",
        "pretrained_model = models.resnet18(pretrained=True)\n",
        "\n",
        "# Step 4: Freeze Pre-trained Model and Add More Fully Connected Layers\n",
        "num_classes = 10  # Number of classes in CIFAR-10\n",
        "num_ftrs = pretrained_model.fc.in_features\n",
        "\n",
        "pretrained_model.fc = nn.Linear(num_ftrs, num_classes)\n",
        "\n",
        "trainable_layer_names = [\"layer3\", \"layer4\", \"avgpool\", \"fc\"]\n",
        "\n",
        "# Iterate through all the modules of the model\n",
        "for name, module in pretrained_model.named_children():\n",
        "    # Check if the module name is in the list of trainable_layer_names\n",
        "    if name in trainable_layer_names:\n",
        "        # If the module name is in the list, set requires_grad to True for all its parameters\n",
        "        for param in module.parameters():\n",
        "            param.requires_grad = True\n",
        "    else:\n",
        "        # If the module name is not in the list, freeze its parameters\n",
        "        for param in module.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = pretrained_model.to(device)\n",
        "print(device)\n",
        "\n",
        "# Step 5: Define Loss Function and Optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(pretrained_model.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# Step 6: Training Loop\n",
        "num_epochs = 5\n",
        "\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        inputs, labels = data[0].to(device), data[1].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = pretrained_model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        if i % 100 == 99:\n",
        "            print('[%d, %5d] loss: %.3f' %\n",
        "                  (epoch + 1, i + 1, running_loss / 100))\n",
        "            running_loss = 0.0\n",
        "\n",
        "print(\"Finished Training\")\n",
        "\n",
        "\n",
        "\n",
        "# Step 7: Evaluation\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data[0].to(device), data[1].to(device)\n",
        "        outputs = pretrained_model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print('Accuracy of the network on the 10000 test images: %d %%' % (100 * correct / total))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qWW6kDrIXSeM",
        "outputId": "b4c4186d-6414-4f2c-c5af-a15694a208f2"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n",
            "[1,   100] loss: 1.470\n",
            "[1,   200] loss: 0.706\n",
            "[1,   300] loss: 0.548\n",
            "[1,   400] loss: 0.485\n",
            "[1,   500] loss: 0.423\n",
            "[1,   600] loss: 0.390\n",
            "[1,   700] loss: 0.354\n",
            "[1,   800] loss: 0.357\n",
            "[1,   900] loss: 0.334\n",
            "[1,  1000] loss: 0.332\n",
            "[1,  1100] loss: 0.310\n",
            "[1,  1200] loss: 0.317\n",
            "[1,  1300] loss: 0.277\n",
            "[1,  1400] loss: 0.312\n",
            "[1,  1500] loss: 0.289\n",
            "[2,   100] loss: 0.193\n",
            "[2,   200] loss: 0.184\n",
            "[2,   300] loss: 0.181\n",
            "[2,   400] loss: 0.186\n",
            "[2,   500] loss: 0.195\n",
            "[2,   600] loss: 0.167\n",
            "[2,   700] loss: 0.197\n",
            "[2,   800] loss: 0.186\n",
            "[2,   900] loss: 0.199\n",
            "[2,  1000] loss: 0.191\n",
            "[2,  1100] loss: 0.168\n",
            "[2,  1200] loss: 0.195\n",
            "[2,  1300] loss: 0.175\n",
            "[2,  1400] loss: 0.184\n",
            "[2,  1500] loss: 0.179\n",
            "[3,   100] loss: 0.106\n",
            "[3,   200] loss: 0.097\n",
            "[3,   300] loss: 0.100\n",
            "[3,   400] loss: 0.091\n",
            "[3,   500] loss: 0.094\n",
            "[3,   600] loss: 0.095\n",
            "[3,   700] loss: 0.081\n",
            "[3,   800] loss: 0.093\n",
            "[3,   900] loss: 0.099\n",
            "[3,  1000] loss: 0.096\n",
            "[3,  1100] loss: 0.107\n",
            "[3,  1200] loss: 0.094\n",
            "[3,  1300] loss: 0.104\n",
            "[3,  1400] loss: 0.110\n",
            "[3,  1500] loss: 0.088\n",
            "[4,   100] loss: 0.054\n",
            "[4,   200] loss: 0.048\n",
            "[4,   300] loss: 0.050\n",
            "[4,   400] loss: 0.046\n",
            "[4,   500] loss: 0.054\n",
            "[4,   600] loss: 0.051\n",
            "[4,   700] loss: 0.063\n",
            "[4,   800] loss: 0.048\n",
            "[4,   900] loss: 0.050\n",
            "[4,  1000] loss: 0.048\n",
            "[4,  1100] loss: 0.059\n",
            "[4,  1200] loss: 0.063\n",
            "[4,  1300] loss: 0.059\n",
            "[4,  1400] loss: 0.054\n",
            "[4,  1500] loss: 0.057\n",
            "[5,   100] loss: 0.032\n",
            "[5,   200] loss: 0.030\n",
            "[5,   300] loss: 0.028\n",
            "[5,   400] loss: 0.033\n",
            "[5,   500] loss: 0.032\n",
            "[5,   600] loss: 0.032\n",
            "[5,   700] loss: 0.030\n",
            "[5,   800] loss: 0.031\n",
            "[5,   900] loss: 0.031\n",
            "[5,  1000] loss: 0.035\n",
            "[5,  1100] loss: 0.030\n",
            "[5,  1200] loss: 0.032\n",
            "[5,  1300] loss: 0.034\n",
            "[5,  1400] loss: 0.030\n",
            "[5,  1500] loss: 0.032\n",
            "Finished Training\n",
            "Accuracy of the network on the 10000 test images: 91 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VFntekVPyJqL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}